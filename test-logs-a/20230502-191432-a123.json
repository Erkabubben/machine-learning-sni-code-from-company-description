{
  "input_parameters": {
    "name_of_data_file": "nlp_applied_data_0",
    "name_of_custom_stopwords_file": "custom_stopwords_0",
    "data_limit": 900000,
    "use_full_SNI_numbers": false,
    "random_state": 42,
    "naive_bayes": {
      "on": true,
      "alpha": 0.017
    },
    "svm": {
      "on": false,
      "C": 1,
      "class_weight": "balanced"
    },
    "pipeline": {
      "on": false
    },
    "split_training_and_testing_data": {
      "test_size": 0.15,
      "stratify": true
    },
    "under_sampling": {
      "on": false,
      "sampling_strategy": "all"
    },
    "filter_low_occuring_labels": {
      "on": true
    },
    "random_oversampling": {
      "on": false,
      "sampling_strategy": "all"
    },
    "SMOTE": {
      "on": true,
      "k_neighbors": 3,
      "sampling_strategy": "not minority"
    },
    "combine_under_and_oversampling": {
      "on": false,
      "over_sampling_strategy": "minority",
      "under_sampling_strategy": "majority"
    },
    "testing_methods": {
      "train_test_split": true,
      "5-fold": false
    }
  },
  "model_test_results": {
    "naive_bayes": {
      "split": {
        "accuracy": 0.5051585495372478,
        "classification_report": "              precision    recall  f1-score   support\n\n           0       0.54      0.66      0.60      1091\n           1       0.53      0.70      0.60       582\n           2       0.35      0.71      0.47        63\n           4       0.07      0.60      0.12         5\n           5       0.17      0.55      0.26        58\n           6       0.23      0.50      0.32        20\n           7       0.27      0.57      0.37       399\n           8       0.35      0.73      0.47       100\n           9       0.13      0.83      0.22         6\n          10       0.13      0.33      0.19        93\n          11       0.04      0.32      0.07        59\n          12       0.06      0.36      0.10        25\n          13       0.29      0.52      0.38       340\n          14       0.14      0.39      0.20        41\n          15       0.29      0.49      0.37       225\n          16       0.00      0.00      0.00         4\n          17       0.11      0.28      0.16        99\n          18       0.05      0.37      0.08        19\n          19       0.23      0.37      0.29       160\n          20       0.22      0.50      0.31       119\n          21       0.08      0.33      0.13        45\n          22       0.46      0.53      0.49       931\n          23       0.12      0.32      0.17       172\n          24       0.09      0.30      0.14       112\n          25       0.14      0.27      0.19       345\n          26       0.10      0.25      0.14       111\n          27       0.13      0.35      0.19        91\n          28       0.16      0.43      0.24       190\n          29       0.24      0.38      0.29       278\n          30       0.24      0.42      0.31       563\n          31       0.46      0.76      0.58       295\n          32       0.09      0.21      0.13        14\n          33       0.12      0.38      0.19        34\n          34       0.25      0.45      0.33       110\n          35       0.02      0.12      0.03        16\n          36       0.42      0.54      0.47      2735\n          37       0.10      0.31      0.15       366\n          38       0.70      0.42      0.53      7547\n          39       0.71      0.71      0.71      2267\n          40       0.39      0.22      0.28      4783\n          41       0.61      0.41      0.49      5297\n          42       0.74      0.65      0.69      2229\n          43       0.30      0.58      0.40       121\n          44       0.21      0.49      0.30        37\n          45       0.32      0.42      0.36       459\n          46       0.15      0.48      0.23        54\n          47       0.47      0.64      0.54       655\n          48       0.80      0.75      0.78      3068\n          49       0.28      0.37      0.32       800\n          50       0.55      0.60      0.57       996\n          51       0.03      0.21      0.05        19\n          52       0.13      0.44      0.20       113\n          53       0.63      0.50      0.56      4879\n          54       0.14      0.28      0.19       421\n          55       0.35      0.41      0.38      3714\n          56       0.63      0.81      0.71        27\n          57       0.26      0.44      0.33       758\n          58       0.73      0.67      0.70      9195\n          59       0.72      0.74      0.73      3232\n          60       0.60      0.35      0.44      9407\n          61       0.56      0.44      0.50      4653\n          62       0.27      0.48      0.35       469\n          63       0.33      0.44      0.38      1203\n          64       0.38      0.34      0.36      2038\n          65       0.53      0.87      0.66       130\n          66       0.22      0.32      0.26       749\n          67       0.39      0.44      0.41      1015\n          68       0.36      0.61      0.45       284\n          69       0.21      0.54      0.30       112\n          70       0.51      0.61      0.56      1378\n          71       0.17      0.25      0.20       397\n          72       0.01      0.09      0.02        11\n          73       0.40      0.46      0.43      1718\n          74       0.79      0.76      0.78      3209\n          75       0.44      0.67      0.53       242\n          76       0.45      0.59      0.51       327\n          77       0.46      0.55      0.50      1234\n          78       0.08      0.23      0.12        31\n          79       0.09      0.68      0.16        34\n          80       0.43      0.56      0.48       976\n          81       0.09      0.27      0.14        37\n          82       0.15      0.41      0.22       167\n          83       0.70      0.70      0.70      1866\n\n    accuracy                           0.51     92274\n   macro avg       0.31      0.47      0.35     92274\nweighted avg       0.56      0.51      0.52     92274\n"
      },
      "5_fold_cv": {
        "accuracy": 0.5532188478407174,
        "classification_report": "              precision    recall  f1-score   support\n\n           0       0.58      0.64      0.61      7276\n           1       0.71      0.54      0.61      3878\n           2       0.83      0.35      0.49       423\n           4       0.00      0.00      0.00        33\n           5       0.63      0.18      0.28       384\n           6       0.64      0.21      0.31       136\n           7       0.48      0.31      0.38      2658\n           8       0.71      0.31      0.43       670\n           9       0.00      0.00      0.00        43\n          10       0.42      0.08      0.14       622\n          11       0.09      0.00      0.00       395\n          12       0.60      0.02      0.03       167\n          13       0.53      0.26      0.35      2268\n          14       0.55      0.02      0.04       276\n          15       0.58      0.32      0.41      1503\n          16       0.00      0.00      0.00        29\n          17       0.24      0.03      0.05       658\n          18       0.00      0.00      0.00       127\n          19       0.51      0.16      0.25      1067\n          20       0.64      0.17      0.27       791\n          21       0.26      0.02      0.03       303\n          22       0.51      0.52      0.52      6207\n          23       0.17      0.03      0.06      1146\n          24       0.18      0.02      0.03       745\n          25       0.24      0.08      0.12      2301\n          26       0.38      0.03      0.05       739\n          27       0.39      0.04      0.07       604\n          28       0.46      0.09      0.15      1266\n          29       0.54      0.17      0.26      1856\n          30       0.36      0.21      0.27      3755\n          31       0.59      0.58      0.58      1966\n          32       0.57      0.18      0.28        92\n          33       0.49      0.10      0.17       229\n          34       0.52      0.20      0.29       732\n          35       0.00      0.00      0.00       107\n          36       0.44      0.29      0.35     18236\n          37       0.31      0.07      0.12      2439\n          38       0.57      0.74      0.64     50310\n          39       0.66      0.76      0.70     15116\n          40       0.37      0.40      0.38     31886\n          41       0.55      0.61      0.58     35313\n          42       0.72      0.74      0.73     14857\n          43       0.58      0.29      0.38       810\n          44       0.58      0.07      0.13       245\n          45       0.51      0.25      0.34      3058\n          46       0.64      0.07      0.13       357\n          47       0.58      0.42      0.49      4364\n          48       0.73      0.81      0.77     20450\n          49       0.41      0.18      0.25      5334\n          50       0.57      0.57      0.57      6638\n          51       0.00      0.00      0.00       129\n          52       0.46      0.11      0.18       753\n          53       0.57      0.59      0.58     32525\n          54       0.25      0.09      0.14      2804\n          55       0.38      0.25      0.30     24761\n          56       0.97      0.81      0.88       182\n          57       0.61      0.30      0.40      5054\n          58       0.59      0.81      0.68     61298\n          59       0.75      0.68      0.72     21544\n          60       0.46      0.58      0.52     62710\n          61       0.53      0.52      0.52     31019\n          62       0.45      0.31      0.37      3127\n          63       0.42      0.33      0.37      8021\n          64       0.42      0.32      0.36     13585\n          65       0.89      0.62      0.73       865\n          66       0.33      0.12      0.17      4991\n          67       0.53      0.29      0.37      6764\n          68       0.61      0.36      0.45      1893\n          69       0.64      0.32      0.43       748\n          70       0.61      0.51      0.56      9186\n          71       0.38      0.12      0.18      2646\n          72       0.00      0.00      0.00        73\n          73       0.48      0.39      0.43     11450\n          74       0.73      0.82      0.77     21395\n          75       0.64      0.51      0.57      1615\n          76       0.62      0.52      0.56      2180\n          77       0.51      0.54      0.52      8230\n          78       0.47      0.04      0.07       208\n          79       0.53      0.04      0.08       230\n          80       0.49      0.48      0.49      6506\n          81       0.25      0.13      0.17       247\n          82       0.50      0.12      0.19      1116\n          83       0.72      0.69      0.70     12438\n\n    accuracy                           0.55    615158\n   macro avg       0.47      0.29      0.33    615158\nweighted avg       0.54      0.55      0.54    615158\n"
      }
    }
  }
}