{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a76140b6fd04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m  \u001b[1;31m# a list of the arguments provided (str)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Initiate logged_results dictionary\n",
    "logged_results = {}\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data/{}.csv\".format(args['name_of_data_file']))\n",
    "\n",
    "# Remove NaN\n",
    "np_data = df.fillna(' ').values\n",
    "\n",
    "# Remove unneccessary columns from dataframe\n",
    "company_descriptions = np_data[0:args['data_limit'], 0]\n",
    "sni_numbers_column = np_data[0:args['data_limit'], (1 if args['use_full_SNI_numbers'] else 3)]\n",
    "\n",
    "# Split data into X and y\n",
    "X_raw = company_descriptions\n",
    "\n",
    "###\n",
    "\n",
    "# The following nltk packages need to be installed, uncomment to install:\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Initiate lemmatizer and stemmer used in NLP operations\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Applies NLP operations to the provided input data and returns a new input data object.\n",
    "# Only need to be done once if the processed dataset is saved to a new CSV file and\n",
    "# then loaded at the start of the script.\n",
    "def ApplyNLPtoInputTextData(X, print_index_interval):\n",
    "\n",
    "    # Takes a single input data string and applies NLP operations.\n",
    "    def Preprocess(sentence, tokenizer):\n",
    "        sentence=str(sentence)\n",
    "        sentence = sentence.lower() # Converts all characters to lowercase\n",
    "        cleanr = re.compile('<.*?>') # Removes special signs\n",
    "        cleantext = re.sub(cleanr, '', sentence) \n",
    "        rem_num = re.sub('[0-9]+', '', cleantext) # Removes numbers\n",
    "        tokens = tokenizer.tokenize(rem_num) # Tokenizes the string\n",
    "        # Removes the general stopwords defined in nltk's Swedish stopwords list\n",
    "        filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('swedish')]\n",
    "        stem_words=[stemmer.stem(w) for w in filtered_words] # Applies stemming\n",
    "        lemma_words=[lemmatizer.lemmatize(w) for w in stem_words] # Applies lemmatization\n",
    "        return \" \".join(filtered_words) # Joins the token array and returns it as a space-separated string\n",
    "\n",
    "    print(\"Applying NLP to input data...\")\n",
    "    time_before_operation = time.perf_counter()\n",
    "    X_NLP_applied = np.empty_like(X)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # Iterates all rows in the input data and applies Preprocess\n",
    "    for index, company_description in enumerate(X):\n",
    "        X_NLP_applied[index] = Preprocess(company_description, tokenizer)\n",
    "        if index % print_index_interval == print_index_interval - 1:\n",
    "            total_time = time.perf_counter() - time_before_operation\n",
    "            print(\"Reached index {} ({} seconds)\".format(index, GetTimeOfOperation(time_before_operation)))\n",
    "    print(\"Finished applying NLP to input data ({} seconds)\".format(GetTimeOfOperation(time_before_operation)))\n",
    "    return X_NLP_applied\n",
    "\n",
    "# Saves a new CSV file where the input data column has been replaced (all other columns are\n",
    "# copied from the original data).\n",
    "def OutputCSVWithReplacedXColumn(np_data, X_raw):\n",
    "    time_before_operation = time.perf_counter()\n",
    "    cropped_np_data = np_data[0:len(X_raw)]\n",
    "    cropped_np_data[:, 0] = X_raw #b[:, j]\n",
    "    # Convert array into dataframe\n",
    "    new_DF = pd.DataFrame(cropped_np_data)\n",
    "    # Save the dataframe as a CSV file\n",
    "    new_csv_name = \"data/new_data.csv\"\n",
    "    new_DF.to_csv(new_csv_name, index=None)\n",
    "    print(\"Finished output of CSV file '{}' ({} seconds)\".format(new_csv_name, GetTimeOfOperation(time_before_operation)))\n",
    "\n",
    "# Only applies NLP preprocessing and outputs a new CSV file if name_of_data_file is the\n",
    "# original dataset (otherwise it assumes that the provided dataset has already been processed).\n",
    "if args['name_of_data_file'] == \"activitytext_sni\":\n",
    "    X_raw = ApplyNLPtoInputTextData(X_raw, 5000)\n",
    "    OutputCSVWithReplacedXColumn(np_data, X_raw)\n",
    "\n",
    "# Function used to output a list of all the most common words in the input data.\n",
    "# Only used once to facilitate creating a custom stopwords list.\n",
    "def OutputListOfMostCommonWords(X, top_range, print_index_interval):\n",
    "\n",
    "    def OutputListToTXTFile(lines):\n",
    "        with open('data/top_words_list.txt', 'w') as f:\n",
    "            for line in lines:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    time_before_operation = time.perf_counter()\n",
    "    word_dictionary = {}\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for index, row in enumerate(X):\n",
    "        tokens = tokenizer.tokenize(row)\n",
    "        for token in tokens:\n",
    "            if token in word_dictionary:\n",
    "                word_dictionary[token] = word_dictionary[token] + 1\n",
    "            else:\n",
    "                word_dictionary[token] = 0\n",
    "        if index % print_index_interval == print_index_interval - 1:\n",
    "            print(\"Reached index {} ({} seconds)\".format(index, GetTimeOfOperation(time_before_operation)))\n",
    "    top_list_for_prints = []\n",
    "    top_list_for_file_output = []\n",
    "    print(\"Ordering list of top words...)\")\n",
    "    for x in range(top_range):\n",
    "        maxword = max(word_dictionary, key=word_dictionary.get)\n",
    "        top_list_for_prints.append(maxword + ': ' + str(word_dictionary[maxword]))\n",
    "        top_list_for_file_output.append(maxword)\n",
    "        del word_dictionary[maxword]\n",
    "    print(top_list_for_prints)\n",
    "    OutputListToTXTFile(top_list_for_prints)\n",
    "    print(\"Finished output of custom stopwords TXT file '{}' ({} seconds)\".format('top_words_list', GetTimeOfOperation(time_before_operation)))\n",
    "\n",
    "# Takes a custom stopwords list file and removes all words that are not prefixed with\n",
    "# a hashtag from the input data.\n",
    "def RemoveCustomStopwords(X, name_of_custom_stopwords_file, print_index_interval):\n",
    "\n",
    "    def RemoveCustomStopwordsFromXRow(company_description, custom_stopwords_list, tokenizer):\n",
    "        tokens = tokenizer.tokenize(company_description)\n",
    "        filtered_words = [w for w in tokens if len(w) > 2 if not w in custom_stopwords_list]\n",
    "        return \" \".join(filtered_words)\n",
    "\n",
    "    # opening the file in read mode\n",
    "    my_file = open('data/' + name_of_custom_stopwords_file + '.txt', \"r\")\n",
    "    # reading the file\n",
    "    data = my_file.read()\n",
    "    custom_stopwords_list_raw = data.split('\\n')\n",
    "    custom_stopwords_list = []\n",
    "    for word in custom_stopwords_list_raw:\n",
    "        if len(word) > 0 and not word.startswith('#'):\n",
    "            custom_stopwords_list.append(word.split(':')[0])\n",
    "    print(\"Removing custom stopwords from input data...\")\n",
    "    time_before_operation = time.perf_counter()\n",
    "    X_out = np.empty_like(X)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for index, company_description in enumerate(X):\n",
    "        X_out[index] = RemoveCustomStopwordsFromXRow(company_description, custom_stopwords_list, tokenizer)\n",
    "        if index % print_index_interval == print_index_interval - 1:\n",
    "            print(\"Reached index {} ({} seconds)\".format(index, GetTimeOfOperation(time_before_operation)))\n",
    "    print(\"Finished removing custom stopwords ({} seconds)\".format(GetTimeOfOperation(time_before_operation)))\n",
    "    return X_out\n",
    "\n",
    "# Utility function for measuring how long an operation takes to finish.\n",
    "def GetTimeOfOperation(time_before_operation):\n",
    "    return time.perf_counter() - time_before_operation\n",
    "\n",
    "# Uncomment to output a list of the most common words in the input data.\n",
    "#OutputListOfMostCommonWords(X_raw, 10000, 1000)\n",
    "\n",
    "# Removes custom stopwords from input data if a custom stopwords list has been assigned.\n",
    "if args['name_of_custom_stopwords_file']:\n",
    "    X_raw = RemoveCustomStopwords(X_raw, args['name_of_custom_stopwords_file'], 100000)\n",
    "\n",
    "# Uncomment to output a new CSV file with a replaced input data column.\n",
    "#OutputCSVWithReplacedXColumn(np_data, X_raw)\n",
    "\n",
    "# Convert class label strings to integers.\n",
    "y_raw = sni_numbers_column\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_raw)\n",
    "y = encoder.transform(y_raw)\n",
    "\n",
    "# Flatten input matrix to vector.\n",
    "X_raw = X_raw.ravel()\n",
    "#print(\"Examples: {}\".format(X_raw.shape[0]))\n",
    "#print(\"Possible categories:\",np.unique(y_raw),\"encoded to\",np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter low occuring labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get the error 'ModuleNotFoundError: No module named 'numpy_indexed',\n",
    "# open the Anaconda Prompt and run the following command:\n",
    "# 'conda install numpy-indexed -c conda-forge'\n",
    "import numpy_indexed as npi\n",
    "if args['filter_low_occuring_labels']['on']:\n",
    "    print('Initiating filtering of low occuring labels...')\n",
    "    train_y = y\n",
    "    train_X = X_raw\n",
    "    samples_mask = npi.multiplicity(train_y) >= 10\n",
    "    y= train_y[samples_mask]\n",
    "    X_raw = train_X[samples_mask]\n",
    "else:\n",
    "    print('Skipped filtering of low occuring labels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615158, 159515)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(X_raw)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from occurencies to frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615158, 159515)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer().fit(X)\n",
    "X = tf_transformer.transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 60 58 ... 56 55 56]\n",
      "Counter({60: 62710, 58: 61298, 38: 50310, 41: 35313, 53: 32525, 40: 31886, 61: 31019, 55: 24761, 59: 21544, 74: 21395, 48: 20450, 36: 18236, 39: 15116, 42: 14857, 64: 13585, 83: 12438, 73: 11450, 70: 9186, 77: 8230, 63: 8021, 0: 7276, 67: 6764, 50: 6638, 80: 6506, 22: 6207, 49: 5334, 57: 5054, 66: 4991, 47: 4364, 1: 3878, 30: 3755, 62: 3127, 45: 3058, 54: 2804, 7: 2658, 71: 2646, 37: 2439, 25: 2301, 13: 2268, 76: 2180, 31: 1966, 68: 1893, 29: 1856, 75: 1615, 15: 1503, 28: 1266, 23: 1146, 82: 1116, 19: 1067, 65: 865, 43: 810, 20: 791, 52: 753, 69: 748, 24: 745, 26: 739, 34: 732, 8: 670, 17: 658, 10: 622, 27: 604, 2: 423, 11: 395, 5: 384, 46: 357, 21: 303, 14: 276, 81: 247, 44: 245, 79: 230, 33: 229, 78: 208, 56: 182, 12: 167, 6: 136, 51: 129, 18: 127, 35: 107, 32: 92, 72: 73, 9: 43, 4: 33, 16: 29})\n",
      "full y\n",
      "Counter({60: 50037, 58: 49057, 38: 40338, 41: 28255, 53: 26002, 40: 25451, 61: 24771, 55: 19920, 59: 17252, 74: 17160, 48: 16359, 36: 14537, 39: 12094, 42: 11894, 64: 10869, 83: 9965, 73: 9146, 70: 7327, 77: 6610, 63: 6389, 0: 5831, 67: 5389, 50: 5324, 80: 5167, 22: 4895, 49: 4299, 57: 4083, 66: 4011, 47: 3552, 1: 3066, 30: 3008, 62: 2462, 45: 2437, 54: 2254, 71: 2135, 7: 2128, 37: 1930, 25: 1858, 13: 1820, 76: 1740, 31: 1597, 68: 1511, 29: 1470, 75: 1298, 15: 1229, 28: 1020, 23: 929, 82: 890, 19: 815, 65: 689, 43: 674, 20: 620, 69: 612, 52: 598, 26: 589, 24: 582, 34: 582, 8: 542, 17: 527, 10: 507, 27: 481, 2: 343, 5: 319, 11: 311, 46: 280, 21: 252, 14: 210, 81: 198, 44: 197, 79: 182, 33: 178, 78: 167, 56: 144, 12: 133, 6: 115, 18: 110, 51: 97, 35: 85, 32: 76, 72: 59, 9: 37, 4: 26, 16: 23})\n",
      "trained y \n",
      "(492126, 159515) (123032, 159515) (492126,) (123032,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "print(y)\n",
    "print(Counter(y))\n",
    "print('full y')\n",
    "\n",
    "if args[\"split_training_and_testing_data\"][\"stratify\"]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args[\"split_training_and_testing_data\"][\"test_size\"], stratify=y, random_state=args[\"random_state\"])\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args[\"split_training_and_testing_data\"][\"test_size\"], random_state=args[\"random_state\"])\n",
    "\n",
    "print(Counter(y_train))\n",
    "print('trained y ')\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get the error 'ModuleNotFoundError: No module named 'numpy_indexed',\n",
    "# open the Anaconda Prompt and run the following command:\n",
    "# 'conda install imblearn.under_sampling -c conda-forge'\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "block_name = 'under_sampling'\n",
    "if args[block_name]['on']:\n",
    "    print('Initiating {}...'.format(block_name))\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    #y_new = y.reshape(-1,1)\n",
    "\n",
    "    #print(y_new.shape)\n",
    "    #print(Counter(X))\n",
    "    undersample = RandomUnderSampler(sampling_strategy=args['under_sampling'][\"sampling_strategy\"], random_state=args[\"random_state\"])\n",
    "    X, y = undersample.fit_resample(X, y.ravel())\n",
    "    print(Counter(y))\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    #print(Counter(y_under))\n",
    "    print('Finished {}.'.format(block_name))\n",
    "else:\n",
    "    print('Skipped {}.'.format(block_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomOverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "block_name = 'random_oversampling'\n",
    "if args[block_name]['on']:\n",
    "    print('Initiating {}...'.format(block_name))\n",
    "    #print(X.shape)\n",
    "    #print(y.shape)\n",
    "\n",
    "    #y_new1 = y.reshape(-1,1)\n",
    "\n",
    "    #print(Counter(y))\n",
    "\n",
    "    ros = RandomOverSampler(sampling_strategy=args['random_oversampling'][\"sampling_strategy\"], random_state=args[\"random_state\"])\n",
    "    X_train, y_train = ros.fit_resample(X_train, y_train.ravel())\n",
    "    print(y_train.shape)\n",
    "    print(Counter(y_train))\n",
    "    print('second round')\n",
    "    X_train, y_train = ros.fit_resample(X_train, y_train.ravel())\n",
    "    print(y_train.shape)\n",
    "    print(Counter(y_train))\n",
    "    print('Finished {}.'.format(block_name))\n",
    "else:\n",
    "    print('Skipped {}.'.format(block_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Under and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "block_name = 'combine_under_and_oversampling'\n",
    "if args[block_name]['on']:\n",
    "    print('Initiating {}...'.format(block_name))\n",
    "    over = RandomOverSampler(sampling_strategy=args['combine_under_and_oversampling'][\"over_sampling_strategy\"])\n",
    "\n",
    "    X_train, y_train = over.fit_resample(X_train, y_train.ravel())\n",
    "\n",
    "    under = RandomUnderSampler(sampling_strategy=args['combine_under_and_oversampling'][\"under_sampling_strategy\"])\n",
    "\n",
    "    X_train, y_train = under.fit_resample(X_train, y_train.ravel())\n",
    "    print('Finished {}.'.format(block_name))\n",
    "else:\n",
    "    print('Skipped {}.'.format(block_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615158, 159515)\n",
      "(615158,)\n",
      "Counter({50: 50037, 41: 50037, 38: 50037, 58: 50037, 67: 50037, 60: 50037, 55: 50037, 61: 50037, 74: 50037, 53: 50037, 1: 50037, 48: 50037, 42: 50037, 59: 50037, 64: 50037, 73: 50037, 30: 50037, 40: 50037, 66: 50037, 0: 50037, 25: 50037, 45: 50037, 76: 50037, 39: 50037, 83: 50037, 70: 50037, 22: 50037, 49: 50037, 47: 50037, 36: 50037, 62: 50037, 77: 50037, 24: 50037, 31: 50037, 63: 50037, 13: 50037, 34: 50037, 23: 50037, 37: 50037, 29: 50037, 8: 50037, 52: 50037, 54: 50037, 19: 50037, 80: 50037, 14: 50037, 71: 50037, 43: 50037, 17: 50037, 28: 50037, 68: 50037, 7: 50037, 10: 50037, 65: 50037, 26: 50037, 75: 50037, 79: 50037, 27: 50037, 15: 50037, 57: 50037, 82: 50037, 35: 50037, 69: 50037, 44: 50037, 20: 50037, 32: 50037, 46: 50037, 5: 50037, 2: 50037, 4: 50037, 56: 50037, 81: 50037, 21: 50037, 12: 50037, 33: 50037, 18: 50037, 11: 50037, 6: 50037, 78: 50037, 9: 50037, 51: 50037, 16: 50037, 72: 50037})\n",
      "(4153071, 159515)\n",
      "(4153071,)\n"
     ]
    }
   ],
   "source": [
    "# If you get the error 'ModuleNotFoundError: No module named 'numpy_indexed',\n",
    "# open the Anaconda Prompt and run the following commands:\n",
    "# 'conda install -c conda-forge imbalanced-learn'\n",
    "# 'conda install imblearn.over_sampling -c conda-forge'\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "block_name = 'SMOTE'\n",
    "if args[block_name]['on']:\n",
    "    print('Initiating {}...'.format(block_name))\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    #k_neighbors=1,\n",
    "    sm = SMOTE(\n",
    "        random_state=args[\"random_state\"],\n",
    "        k_neighbors=args[\"SMOTE\"][\"k_neighbors\"],\n",
    "        sampling_strategy=args['SMOTE'][\"sampling_strategy\"])\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train.ravel())\n",
    "    #X_train, y_train = sm.fit_resample(X, y.ravel())\n",
    "    print(Counter(y_train))\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print('Finished {}.'.format(block_name))\n",
    "else:\n",
    "    print('Skipped {}.'.format(block_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for evaluating model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluateTesting(model, name):\n",
    "    \n",
    "    def AddToLoggedResults(lr, test_type):\n",
    "        lr[test_type] = {}\n",
    "        lr[test_type]['accuracy'] = accuracy\n",
    "        if log_confusion_matrix:\n",
    "            lr[test_type]['confusion_matrix'] = conf_mx.tolist()\n",
    "        if test_type == '5_fold_cv':\n",
    "            lr[test_type]['classification_report'] = classification_report(y, y_pred)\n",
    "        else:\n",
    "            lr[test_type]['classification_report'] = classification_report(y_test, y_pred)\n",
    "    \n",
    "    logged_results[name] = {}\n",
    "\n",
    "    print(\"-- Training data --\")\n",
    "    # train model on training dataset\n",
    "    model.fit(X_train, y_train)\n",
    "    # evaluate dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Y Pred: \",  (y_pred.shape))\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    # confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    conf_mx = confusion_matrix(y_test, y_pred)\n",
    "    print(conf_mx)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    AddToLoggedResults(logged_results[name], 'split')\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"-- 5-fold CV --\")\n",
    "    # 5-fold CV\n",
    "    y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(\"Average accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    # confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    conf_mx = confusion_matrix(y, y_pred)\n",
    "    print(conf_mx)\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    AddToLoggedResults(logged_results[name], '5_fold_cv')\n",
    "    \n",
    "    #logged_results[name]['5_fold_cv'] = {}\n",
    "    #logged_results[name]['5_fold_cv']['accuracy'] = accuracy\n",
    "    #if log_confusion_matrix:\n",
    "    #    logged_results[name]['5_fold_cv']['confusion_matrix'] = conf_mx.tolist()\n",
    "    #logged_results[name]['5_fold_cv']['classification_report'] = classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training data --\n",
      "Y Pred:  (123032,)\n",
      "Accuracy: 50.66%\n",
      "Confusion Matrix:\n",
      "[[ 941   54    2 ...    0    1   17]\n",
      " [  50  551    4 ...    1    0    0]\n",
      " [   2    1   58 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...   10    0    2]\n",
      " [   0    1    0 ...    0   88    0]\n",
      " [  11    3    0 ...    1    7 1713]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.65      0.59      1445\n",
      "           1       0.57      0.68      0.62       812\n",
      "           2       0.40      0.72      0.52        80\n",
      "           4       0.04      0.43      0.07         7\n",
      "           5       0.16      0.55      0.25        65\n",
      "           6       0.09      0.29      0.14        21\n",
      "           7       0.28      0.55      0.37       530\n",
      "           8       0.30      0.72      0.43       128\n",
      "           9       0.11      0.67      0.18         6\n",
      "          10       0.16      0.42      0.23       115\n",
      "          11       0.04      0.30      0.08        84\n",
      "          12       0.05      0.24      0.09        34\n",
      "          13       0.29      0.50      0.37       448\n",
      "          14       0.16      0.38      0.22        66\n",
      "          15       0.31      0.55      0.40       274\n",
      "          16       0.02      0.17      0.04         6\n",
      "          17       0.12      0.27      0.16       131\n",
      "          18       0.03      0.24      0.05        17\n",
      "          19       0.30      0.40      0.34       252\n",
      "          20       0.23      0.43      0.30       171\n",
      "          21       0.04      0.24      0.07        51\n",
      "          22       0.49      0.51      0.50      1312\n",
      "          23       0.10      0.28      0.15       217\n",
      "          24       0.11      0.30      0.17       163\n",
      "          25       0.16      0.32      0.21       443\n",
      "          26       0.13      0.28      0.18       150\n",
      "          27       0.15      0.36      0.21       123\n",
      "          28       0.16      0.42      0.23       246\n",
      "          29       0.26      0.38      0.31       386\n",
      "          30       0.23      0.40      0.30       747\n",
      "          31       0.45      0.72      0.55       369\n",
      "          32       0.11      0.31      0.16        16\n",
      "          33       0.27      0.43      0.33        51\n",
      "          34       0.25      0.45      0.32       150\n",
      "          35       0.03      0.36      0.06        22\n",
      "          36       0.42      0.51      0.46      3699\n",
      "          37       0.12      0.36      0.18       509\n",
      "          38       0.68      0.43      0.53      9972\n",
      "          39       0.70      0.70      0.70      3022\n",
      "          40       0.39      0.23      0.29      6435\n",
      "          41       0.59      0.40      0.48      7058\n",
      "          42       0.75      0.66      0.70      2963\n",
      "          43       0.28      0.58      0.38       136\n",
      "          44       0.18      0.42      0.25        48\n",
      "          45       0.35      0.45      0.39       621\n",
      "          46       0.13      0.31      0.18        77\n",
      "          47       0.46      0.63      0.53       812\n",
      "          48       0.81      0.75      0.78      4091\n",
      "          49       0.26      0.33      0.29      1035\n",
      "          50       0.53      0.56      0.55      1314\n",
      "          51       0.03      0.19      0.05        32\n",
      "          52       0.15      0.47      0.22       155\n",
      "          53       0.63      0.50      0.56      6523\n",
      "          54       0.15      0.28      0.20       550\n",
      "          55       0.35      0.42      0.38      4841\n",
      "          56       0.74      0.76      0.75        38\n",
      "          57       0.27      0.44      0.33       971\n",
      "          58       0.73      0.69      0.71     12241\n",
      "          59       0.70      0.72      0.71      4292\n",
      "          60       0.58      0.37      0.46     12673\n",
      "          61       0.56      0.46      0.51      6248\n",
      "          62       0.30      0.48      0.37       665\n",
      "          63       0.36      0.46      0.40      1632\n",
      "          64       0.36      0.34      0.35      2716\n",
      "          65       0.52      0.75      0.61       176\n",
      "          66       0.21      0.30      0.25       980\n",
      "          67       0.38      0.41      0.40      1375\n",
      "          68       0.39      0.54      0.45       382\n",
      "          69       0.20      0.55      0.30       136\n",
      "          70       0.50      0.60      0.54      1859\n",
      "          71       0.19      0.28      0.22       511\n",
      "          72       0.03      0.14      0.05        14\n",
      "          73       0.40      0.46      0.43      2304\n",
      "          74       0.77      0.77      0.77      4235\n",
      "          75       0.47      0.66      0.55       317\n",
      "          76       0.50      0.61      0.55       440\n",
      "          77       0.45      0.55      0.50      1620\n",
      "          78       0.07      0.15      0.09        41\n",
      "          79       0.07      0.46      0.13        48\n",
      "          80       0.42      0.54      0.47      1339\n",
      "          81       0.09      0.20      0.13        49\n",
      "          82       0.15      0.39      0.22       226\n",
      "          83       0.71      0.69      0.70      2473\n",
      "\n",
      "    accuracy                           0.51    123032\n",
      "   macro avg       0.32      0.46      0.35    123032\n",
      "weighted avg       0.55      0.51      0.52    123032\n",
      "\n",
      "\n",
      "-- 5-fold CV --\n",
      "Average accuracy: 54.53%\n",
      "Confusion Matrix:\n",
      "[[4547   94    1 ...    0    0   73]\n",
      " [ 272 2055    1 ...    1    0    4]\n",
      " [  17    2  144 ...    1    0    0]\n",
      " ...\n",
      " [   1    1    0 ...   44    0    6]\n",
      " [   0    0    0 ...    0  161   25]\n",
      " [  54    5    0 ...    1   18 8386]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60      7276\n",
      "           1       0.71      0.53      0.61      3878\n",
      "           2       0.80      0.34      0.48       423\n",
      "           4       0.00      0.00      0.00        33\n",
      "           5       0.61      0.18      0.28       384\n",
      "           6       0.68      0.17      0.27       136\n",
      "           7       0.46      0.30      0.36      2658\n",
      "           8       0.65      0.34      0.45       670\n",
      "           9       0.00      0.00      0.00        43\n",
      "          10       0.34      0.09      0.14       622\n",
      "          11       0.16      0.01      0.02       395\n",
      "          12       0.50      0.02      0.03       167\n",
      "          13       0.49      0.26      0.34      2268\n",
      "          14       0.42      0.03      0.05       276\n",
      "          15       0.56      0.33      0.41      1503\n",
      "          16       1.00      0.03      0.07        29\n",
      "          17       0.24      0.04      0.07       658\n",
      "          18       0.00      0.00      0.00       127\n",
      "          19       0.46      0.17      0.25      1067\n",
      "          20       0.55      0.17      0.26       791\n",
      "          21       0.27      0.02      0.04       303\n",
      "          22       0.51      0.50      0.50      6207\n",
      "          23       0.17      0.05      0.08      1146\n",
      "          24       0.15      0.02      0.04       745\n",
      "          25       0.23      0.09      0.13      2301\n",
      "          26       0.30      0.03      0.06       739\n",
      "          27       0.30      0.04      0.07       604\n",
      "          28       0.39      0.10      0.16      1266\n",
      "          29       0.49      0.17      0.25      1856\n",
      "          30       0.35      0.21      0.26      3755\n",
      "          31       0.56      0.55      0.56      1966\n",
      "          32       0.51      0.20      0.28        92\n",
      "          33       0.45      0.12      0.19       229\n",
      "          34       0.49      0.23      0.31       732\n",
      "          35       1.00      0.01      0.02       107\n",
      "          36       0.43      0.25      0.31     18236\n",
      "          37       0.29      0.08      0.12      2439\n",
      "          38       0.56      0.74      0.64     50310\n",
      "          39       0.66      0.74      0.70     15116\n",
      "          40       0.37      0.39      0.38     31886\n",
      "          41       0.55      0.60      0.58     35313\n",
      "          42       0.71      0.72      0.72     14857\n",
      "          43       0.52      0.26      0.34       810\n",
      "          44       0.51      0.09      0.15       245\n",
      "          45       0.48      0.24      0.32      3058\n",
      "          46       0.58      0.08      0.14       357\n",
      "          47       0.57      0.40      0.47      4364\n",
      "          48       0.74      0.79      0.77     20450\n",
      "          49       0.40      0.17      0.24      5334\n",
      "          50       0.57      0.55      0.56      6638\n",
      "          51       0.00      0.00      0.00       129\n",
      "          52       0.41      0.13      0.20       753\n",
      "          53       0.57      0.59      0.58     32525\n",
      "          54       0.22      0.11      0.14      2804\n",
      "          55       0.34      0.22      0.26     24761\n",
      "          56       0.94      0.81      0.87       182\n",
      "          57       0.54      0.30      0.39      5054\n",
      "          58       0.56      0.80      0.66     61298\n",
      "          59       0.76      0.67      0.71     21544\n",
      "          60       0.46      0.59      0.52     62710\n",
      "          61       0.53      0.50      0.52     31019\n",
      "          62       0.43      0.31      0.36      3127\n",
      "          63       0.42      0.31      0.36      8021\n",
      "          64       0.41      0.31      0.36     13585\n",
      "          65       0.86      0.61      0.71       865\n",
      "          66       0.31      0.11      0.17      4991\n",
      "          67       0.51      0.26      0.35      6764\n",
      "          68       0.57      0.36      0.44      1893\n",
      "          69       0.60      0.32      0.41       748\n",
      "          70       0.59      0.50      0.54      9186\n",
      "          71       0.34      0.13      0.18      2646\n",
      "          72       0.00      0.00      0.00        73\n",
      "          73       0.47      0.38      0.42     11450\n",
      "          74       0.73      0.81      0.77     21395\n",
      "          75       0.62      0.49      0.55      1615\n",
      "          76       0.60      0.52      0.56      2180\n",
      "          77       0.50      0.53      0.52      8230\n",
      "          78       0.47      0.07      0.12       208\n",
      "          79       0.53      0.03      0.07       230\n",
      "          80       0.48      0.47      0.48      6506\n",
      "          81       0.18      0.18      0.18       247\n",
      "          82       0.48      0.14      0.22      1116\n",
      "          83       0.72      0.67      0.69     12438\n",
      "\n",
      "    accuracy                           0.55    615158\n",
      "   macro avg       0.48      0.29      0.33    615158\n",
      "weighted avg       0.53      0.55      0.53    615158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "block_name = 'naive_bayes'\n",
    "if args[block_name]['on']:\n",
    "    print('Initiating {}...'.format(block_name))\n",
    "    model = MultinomialNB(alpha=args[block_name]['alpha'])\n",
    "    evaluateTesting(model, 'naive_bayes')\n",
    "    print('Finished {}.'.format(block_name))\n",
    "else:\n",
    "    print('Skipped {}.'.format(block_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training data --\n",
      "Y Pred:  (123032,)\n",
      "Accuracy: 49.87%\n",
      "Confusion Matrix:\n",
      "[[ 996   20    1 ...    0    3   12]\n",
      " [  51  575    2 ...    0    1    2]\n",
      " [   3    0   57 ...    0    0    0]\n",
      " ...\n",
      " [   2    1    1 ...    5    0    1]\n",
      " [   2    0    0 ...    1   89    2]\n",
      " [  19    3    0 ...    0    8 1745]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.69      0.56      1445\n",
      "           1       0.57      0.71      0.63       812\n",
      "           2       0.43      0.71      0.54        80\n",
      "           4       0.00      0.00      0.00         7\n",
      "           5       0.13      0.49      0.20        65\n",
      "           6       0.02      0.19      0.03        21\n",
      "           7       0.31      0.59      0.41       530\n",
      "           8       0.40      0.66      0.50       128\n",
      "           9       0.33      0.67      0.44         6\n",
      "          10       0.10      0.37      0.16       115\n",
      "          11       0.06      0.30      0.10        84\n",
      "          12       0.16      0.29      0.21        34\n",
      "          13       0.24      0.46      0.31       448\n",
      "          14       0.07      0.36      0.12        66\n",
      "          15       0.28      0.53      0.37       274\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.14      0.33      0.19       131\n",
      "          18       0.04      0.18      0.07        17\n",
      "          19       0.23      0.39      0.29       252\n",
      "          20       0.24      0.44      0.31       171\n",
      "          21       0.02      0.14      0.04        51\n",
      "          22       0.45      0.52      0.48      1312\n",
      "          23       0.10      0.27      0.15       217\n",
      "          24       0.11      0.27      0.15       163\n",
      "          25       0.17      0.32      0.22       443\n",
      "          26       0.10      0.28      0.15       150\n",
      "          27       0.09      0.36      0.14       123\n",
      "          28       0.15      0.39      0.21       246\n",
      "          29       0.22      0.40      0.28       386\n",
      "          30       0.22      0.40      0.28       747\n",
      "          31       0.48      0.72      0.58       369\n",
      "          32       0.28      0.31      0.29        16\n",
      "          33       0.17      0.33      0.22        51\n",
      "          34       0.18      0.39      0.24       150\n",
      "          35       0.04      0.36      0.07        22\n",
      "          36       0.42      0.52      0.47      3699\n",
      "          37       0.12      0.34      0.18       509\n",
      "          38       0.72      0.45      0.55      9972\n",
      "          39       0.67      0.70      0.69      3022\n",
      "          40       0.45      0.26      0.33      6435\n",
      "          41       0.61      0.46      0.52      7058\n",
      "          42       0.73      0.67      0.70      2963\n",
      "          43       0.26      0.54      0.35       136\n",
      "          44       0.06      0.19      0.09        48\n",
      "          45       0.29      0.45      0.35       621\n",
      "          46       0.09      0.29      0.14        77\n",
      "          47       0.34      0.66      0.45       812\n",
      "          48       0.81      0.78      0.79      4091\n",
      "          49       0.22      0.36      0.27      1035\n",
      "          50       0.50      0.60      0.55      1314\n",
      "          51       0.05      0.12      0.08        32\n",
      "          52       0.11      0.35      0.17       155\n",
      "          53       0.63      0.48      0.55      6523\n",
      "          54       0.12      0.28      0.17       550\n",
      "          55       0.41      0.34      0.37      4841\n",
      "          56       0.09      0.76      0.16        38\n",
      "          57       0.27      0.47      0.34       971\n",
      "          58       0.77      0.60      0.67     12241\n",
      "          59       0.70      0.75      0.72      4292\n",
      "          60       0.65      0.32      0.42     12673\n",
      "          61       0.57      0.46      0.51      6248\n",
      "          62       0.27      0.45      0.34       665\n",
      "          63       0.33      0.46      0.38      1632\n",
      "          64       0.36      0.35      0.35      2716\n",
      "          65       0.77      0.81      0.79       176\n",
      "          66       0.20      0.37      0.26       980\n",
      "          67       0.33      0.46      0.39      1375\n",
      "          68       0.36      0.53      0.43       382\n",
      "          69       0.21      0.51      0.30       136\n",
      "          70       0.47      0.61      0.53      1859\n",
      "          71       0.12      0.27      0.17       511\n",
      "          72       0.03      0.07      0.05        14\n",
      "          73       0.38      0.47      0.42      2304\n",
      "          74       0.77      0.78      0.78      4235\n",
      "          75       0.34      0.60      0.43       317\n",
      "          76       0.41      0.58      0.48       440\n",
      "          77       0.42      0.55      0.48      1620\n",
      "          78       0.10      0.15      0.12        41\n",
      "          79       0.08      0.40      0.13        48\n",
      "          80       0.40      0.53      0.46      1339\n",
      "          81       0.02      0.10      0.04        49\n",
      "          82       0.16      0.39      0.22       226\n",
      "          83       0.67      0.71      0.68      2473\n",
      "\n",
      "    accuracy                           0.50    123032\n",
      "   macro avg       0.30      0.44      0.33    123032\n",
      "weighted avg       0.57      0.50      0.51    123032\n",
      "\n",
      "\n",
      "-- 5-fold CV --\n",
      "Average accuracy: 58.72%\n",
      "Confusion Matrix:\n",
      "[[4997   86    1 ...    0    0   67]\n",
      " [ 247 2495    0 ...    0    1    3]\n",
      " [  12    5  278 ...    1    0    0]\n",
      " ...\n",
      " [   1    1    0 ...   32    0    7]\n",
      " [   2    1    0 ...    0  275   22]\n",
      " [  47    4    0 ...    0   17 8919]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.69      0.65      7276\n",
      "           1       0.67      0.64      0.66      3878\n",
      "           2       0.63      0.66      0.64       423\n",
      "           4       0.25      0.06      0.10        33\n",
      "           5       0.49      0.46      0.47       384\n",
      "           6       0.52      0.49      0.50       136\n",
      "           7       0.51      0.43      0.47      2658\n",
      "           8       0.63      0.57      0.60       670\n",
      "           9       0.46      0.44      0.45        43\n",
      "          10       0.42      0.24      0.30       622\n",
      "          11       0.27      0.06      0.10       395\n",
      "          12       0.41      0.14      0.21       167\n",
      "          13       0.47      0.36      0.41      2268\n",
      "          14       0.37      0.15      0.21       276\n",
      "          15       0.52      0.45      0.48      1503\n",
      "          16       0.17      0.03      0.06        29\n",
      "          17       0.35      0.15      0.21       658\n",
      "          18       0.27      0.06      0.09       127\n",
      "          19       0.44      0.35      0.39      1067\n",
      "          20       0.51      0.37      0.43       791\n",
      "          21       0.30      0.12      0.17       303\n",
      "          22       0.50      0.55      0.52      6207\n",
      "          23       0.29      0.12      0.17      1146\n",
      "          24       0.25      0.10      0.15       745\n",
      "          25       0.29      0.16      0.21      2301\n",
      "          26       0.31      0.08      0.13       739\n",
      "          27       0.36      0.17      0.23       604\n",
      "          28       0.42      0.26      0.32      1266\n",
      "          29       0.46      0.27      0.34      1856\n",
      "          30       0.42      0.29      0.34      3755\n",
      "          31       0.65      0.69      0.67      1966\n",
      "          32       0.50      0.28      0.36        92\n",
      "          33       0.36      0.21      0.27       229\n",
      "          34       0.44      0.30      0.36       732\n",
      "          35       0.26      0.05      0.08       107\n",
      "          36       0.47      0.41      0.44     18236\n",
      "          37       0.34      0.12      0.18      2439\n",
      "          38       0.62      0.71      0.66     50310\n",
      "          39       0.69      0.76      0.72     15116\n",
      "          40       0.42      0.42      0.42     31886\n",
      "          41       0.56      0.62      0.59     35313\n",
      "          42       0.73      0.79      0.76     14857\n",
      "          43       0.49      0.48      0.49       810\n",
      "          44       0.42      0.31      0.36       245\n",
      "          45       0.52      0.35      0.42      3058\n",
      "          46       0.48      0.22      0.30       357\n",
      "          47       0.59      0.61      0.60      4364\n",
      "          48       0.79      0.83      0.81     20450\n",
      "          49       0.42      0.23      0.30      5334\n",
      "          50       0.58      0.62      0.60      6638\n",
      "          51       0.44      0.09      0.14       129\n",
      "          52       0.37      0.20      0.26       753\n",
      "          53       0.59      0.66      0.62     32525\n",
      "          54       0.29      0.10      0.15      2804\n",
      "          55       0.45      0.36      0.40     24761\n",
      "          56       0.95      0.82      0.88       182\n",
      "          57       0.59      0.33      0.43      5054\n",
      "          58       0.66      0.80      0.72     61298\n",
      "          59       0.73      0.75      0.74     21544\n",
      "          60       0.54      0.57      0.55     62710\n",
      "          61       0.55      0.54      0.55     31019\n",
      "          62       0.46      0.36      0.40      3127\n",
      "          63       0.47      0.37      0.41      8021\n",
      "          64       0.45      0.34      0.38     13585\n",
      "          65       0.83      0.86      0.84       865\n",
      "          66       0.39      0.21      0.28      4991\n",
      "          67       0.52      0.36      0.43      6764\n",
      "          68       0.54      0.51      0.52      1893\n",
      "          69       0.54      0.39      0.45       748\n",
      "          70       0.60      0.57      0.58      9186\n",
      "          71       0.39      0.19      0.26      2646\n",
      "          72       0.33      0.03      0.05        73\n",
      "          73       0.52      0.43      0.47     11450\n",
      "          74       0.75      0.85      0.80     21395\n",
      "          75       0.59      0.54      0.57      1615\n",
      "          76       0.61      0.57      0.59      2180\n",
      "          77       0.52      0.56      0.54      8230\n",
      "          78       0.41      0.20      0.27       208\n",
      "          79       0.48      0.18      0.26       230\n",
      "          80       0.51      0.50      0.51      6506\n",
      "          81       0.21      0.13      0.16       247\n",
      "          82       0.48      0.25      0.33      1116\n",
      "          83       0.71      0.72      0.72     12438\n",
      "\n",
      "    accuracy                           0.59    615158\n",
      "   macro avg       0.49      0.39      0.42    615158\n",
      "weighted avg       0.57      0.59      0.58    615158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "block_name = 'svm'\n",
    "if args[block_name]['on']:\n",
    "    print('Initiating {}...'.format(block_name))\n",
    "    model = svm.LinearSVC(\n",
    "        random_state=args[\"random_state\"],\n",
    "        C=args[block_name]['C'],\n",
    "        class_weight=args[block_name]['class_weight'])\n",
    "    evaluateTesting(model, 'svm')\n",
    "    print('Finished {}.'.format(block_name))\n",
    "else:\n",
    "    print('Skipped {}.'.format(block_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615158, 159515)\n",
      "-- Training data --\n",
      "Y Pred:  (123032,)\n",
      "Accuracy: 55.34%\n",
      "Confusion Matrix:\n",
      "[[ 884   16    0 ...    0    0   17]\n",
      " [  58  440    0 ...    1    0    0]\n",
      " [   3    1   33 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...    6    0    1]\n",
      " [   0    0    0 ...    0   35    2]\n",
      " [  12    2    0 ...    0    2 1669]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60      1445\n",
      "           1       0.74      0.54      0.62       812\n",
      "           2       0.89      0.41      0.56        80\n",
      "           4       0.00      0.00      0.00         7\n",
      "           5       0.70      0.22      0.33        65\n",
      "           6       0.60      0.14      0.23        21\n",
      "           7       0.47      0.29      0.36       530\n",
      "           8       0.74      0.34      0.46       128\n",
      "           9       0.00      0.00      0.00         6\n",
      "          10       0.52      0.11      0.19       115\n",
      "          11       0.25      0.01      0.02        84\n",
      "          12       1.00      0.03      0.06        34\n",
      "          13       0.55      0.27      0.36       448\n",
      "          14       0.50      0.02      0.03        66\n",
      "          15       0.58      0.36      0.44       274\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.12      0.01      0.01       131\n",
      "          18       0.00      0.00      0.00        17\n",
      "          19       0.57      0.17      0.26       252\n",
      "          20       0.65      0.18      0.28       171\n",
      "          21       0.17      0.02      0.04        51\n",
      "          22       0.57      0.49      0.53      1312\n",
      "          23       0.21      0.05      0.08       217\n",
      "          24       0.36      0.03      0.06       163\n",
      "          25       0.29      0.09      0.14       443\n",
      "          26       0.42      0.03      0.06       150\n",
      "          27       0.54      0.06      0.10       123\n",
      "          28       0.38      0.08      0.13       246\n",
      "          29       0.70      0.17      0.27       386\n",
      "          30       0.41      0.22      0.29       747\n",
      "          31       0.59      0.58      0.59       369\n",
      "          32       0.12      0.06      0.08        16\n",
      "          33       0.60      0.12      0.20        51\n",
      "          34       0.59      0.19      0.29       150\n",
      "          35       0.00      0.00      0.00        22\n",
      "          36       0.48      0.26      0.34      3699\n",
      "          37       0.40      0.08      0.14       509\n",
      "          38       0.55      0.76      0.64      9972\n",
      "          39       0.65      0.74      0.70      3022\n",
      "          40       0.38      0.40      0.39      6435\n",
      "          41       0.55      0.61      0.58      7058\n",
      "          42       0.73      0.73      0.73      2963\n",
      "          43       0.56      0.29      0.39       136\n",
      "          44       0.40      0.04      0.08        48\n",
      "          45       0.54      0.24      0.34       621\n",
      "          46       0.62      0.13      0.22        77\n",
      "          47       0.57      0.37      0.45       812\n",
      "          48       0.76      0.80      0.78      4091\n",
      "          49       0.46      0.17      0.25      1035\n",
      "          50       0.58      0.55      0.56      1314\n",
      "          51       0.00      0.00      0.00        32\n",
      "          52       0.51      0.12      0.19       155\n",
      "          53       0.58      0.61      0.59      6523\n",
      "          54       0.30      0.09      0.14       550\n",
      "          55       0.35      0.22      0.27      4841\n",
      "          56       0.94      0.76      0.84        38\n",
      "          57       0.64      0.30      0.41       971\n",
      "          58       0.56      0.82      0.66     12241\n",
      "          59       0.77      0.67      0.72      4292\n",
      "          60       0.46      0.60      0.52     12673\n",
      "          61       0.54      0.52      0.53      6248\n",
      "          62       0.48      0.29      0.36       665\n",
      "          63       0.44      0.32      0.37      1632\n",
      "          64       0.43      0.32      0.37      2716\n",
      "          65       0.89      0.65      0.75       176\n",
      "          66       0.33      0.10      0.16       980\n",
      "          67       0.55      0.26      0.35      1375\n",
      "          68       0.66      0.33      0.44       382\n",
      "          69       0.54      0.33      0.41       136\n",
      "          70       0.62      0.50      0.55      1859\n",
      "          71       0.42      0.12      0.19       511\n",
      "          72       0.00      0.00      0.00        14\n",
      "          73       0.49      0.38      0.43      2304\n",
      "          74       0.73      0.82      0.78      4235\n",
      "          75       0.69      0.56      0.62       317\n",
      "          76       0.67      0.51      0.58       440\n",
      "          77       0.51      0.54      0.52      1620\n",
      "          78       0.50      0.02      0.05        41\n",
      "          79       0.71      0.10      0.18        48\n",
      "          80       0.52      0.46      0.49      1339\n",
      "          81       0.30      0.12      0.17        49\n",
      "          82       0.54      0.15      0.24       226\n",
      "          83       0.72      0.67      0.70      2473\n",
      "\n",
      "    accuracy                           0.55    123032\n",
      "   macro avg       0.49      0.29      0.34    123032\n",
      "weighted avg       0.55      0.55      0.53    123032\n",
      "\n",
      "\n",
      "-- 5-fold CV --\n",
      "Average accuracy: 54.63%\n",
      "Confusion Matrix:\n",
      "[[4492   91    0 ...    0    0   72]\n",
      " [ 270 2036    1 ...    1    0    3]\n",
      " [  14    1  142 ...    1    0    0]\n",
      " ...\n",
      " [   1    1    0 ...   35    0    6]\n",
      " [   0    0    0 ...    0  150   26]\n",
      " [  52    3    0 ...    0   14 8352]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.62      0.60      7276\n",
      "           1       0.73      0.53      0.61      3878\n",
      "           2       0.80      0.34      0.47       423\n",
      "           4       0.00      0.00      0.00        33\n",
      "           5       0.62      0.20      0.30       384\n",
      "           6       0.71      0.18      0.28       136\n",
      "           7       0.50      0.29      0.37      2658\n",
      "           8       0.70      0.34      0.46       670\n",
      "           9       0.00      0.00      0.00        43\n",
      "          10       0.38      0.08      0.13       622\n",
      "          11       0.17      0.01      0.01       395\n",
      "          12       0.75      0.02      0.04       167\n",
      "          13       0.53      0.25      0.34      2268\n",
      "          14       0.46      0.02      0.04       276\n",
      "          15       0.59      0.32      0.42      1503\n",
      "          16       0.00      0.00      0.00        29\n",
      "          17       0.31      0.03      0.05       658\n",
      "          18       0.00      0.00      0.00       127\n",
      "          19       0.53      0.16      0.24      1067\n",
      "          20       0.63      0.15      0.25       791\n",
      "          21       0.35      0.03      0.05       303\n",
      "          22       0.53      0.49      0.51      6207\n",
      "          23       0.22      0.03      0.06      1146\n",
      "          24       0.25      0.02      0.04       745\n",
      "          25       0.26      0.07      0.12      2301\n",
      "          26       0.33      0.03      0.05       739\n",
      "          27       0.35      0.04      0.07       604\n",
      "          28       0.44      0.09      0.15      1266\n",
      "          29       0.58      0.16      0.25      1856\n",
      "          30       0.38      0.20      0.26      3755\n",
      "          31       0.60      0.55      0.57      1966\n",
      "          32       0.51      0.22      0.31        92\n",
      "          33       0.51      0.11      0.18       229\n",
      "          34       0.51      0.21      0.29       732\n",
      "          35       0.00      0.00      0.00       107\n",
      "          36       0.44      0.24      0.31     18236\n",
      "          37       0.34      0.07      0.11      2439\n",
      "          38       0.55      0.75      0.64     50310\n",
      "          39       0.67      0.74      0.70     15116\n",
      "          40       0.37      0.40      0.38     31886\n",
      "          41       0.55      0.61      0.57     35313\n",
      "          42       0.72      0.72      0.72     14857\n",
      "          43       0.55      0.25      0.35       810\n",
      "          44       0.58      0.07      0.13       245\n",
      "          45       0.51      0.23      0.32      3058\n",
      "          46       0.61      0.10      0.16       357\n",
      "          47       0.60      0.39      0.47      4364\n",
      "          48       0.76      0.79      0.78     20450\n",
      "          49       0.44      0.16      0.23      5334\n",
      "          50       0.58      0.55      0.56      6638\n",
      "          51       0.00      0.00      0.00       129\n",
      "          52       0.48      0.12      0.19       753\n",
      "          53       0.57      0.59      0.58     32525\n",
      "          54       0.27      0.09      0.13      2804\n",
      "          55       0.35      0.21      0.27     24761\n",
      "          56       0.96      0.81      0.88       182\n",
      "          57       0.57      0.30      0.39      5054\n",
      "          58       0.56      0.81      0.66     61298\n",
      "          59       0.77      0.67      0.71     21544\n",
      "          60       0.45      0.61      0.52     62710\n",
      "          61       0.52      0.51      0.52     31019\n",
      "          62       0.48      0.29      0.36      3127\n",
      "          63       0.43      0.30      0.35      8021\n",
      "          64       0.42      0.31      0.36     13585\n",
      "          65       0.87      0.60      0.71       865\n",
      "          66       0.34      0.11      0.17      4991\n",
      "          67       0.53      0.26      0.35      6764\n",
      "          68       0.62      0.34      0.44      1893\n",
      "          69       0.63      0.32      0.42       748\n",
      "          70       0.61      0.49      0.54      9186\n",
      "          71       0.41      0.12      0.18      2646\n",
      "          72       0.00      0.00      0.00        73\n",
      "          73       0.48      0.37      0.42     11450\n",
      "          74       0.73      0.81      0.77     21395\n",
      "          75       0.63      0.48      0.55      1615\n",
      "          76       0.63      0.52      0.57      2180\n",
      "          77       0.51      0.52      0.51      8230\n",
      "          78       0.61      0.05      0.10       208\n",
      "          79       0.56      0.04      0.08       230\n",
      "          80       0.49      0.46      0.48      6506\n",
      "          81       0.26      0.14      0.18       247\n",
      "          82       0.53      0.13      0.21      1116\n",
      "          83       0.72      0.67      0.69     12438\n",
      "\n",
      "    accuracy                           0.55    615158\n",
      "   macro avg       0.48      0.29      0.33    615158\n",
      "weighted avg       0.54      0.55      0.53    615158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "\n",
    "block_name = 'pipeline'\n",
    "if args[block_name]['on']:\n",
    "    print(X.shape)\n",
    "    #print(X_raw.shape)\n",
    "    X = X_raw.ravel()\n",
    "    #X = X.ravel()\n",
    "    #print(X.shape)\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=args[\"random_state\"])\n",
    "\n",
    "    #how do i get in SMOTE in pipeline, just get error: lower not found. \n",
    "    model = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB(alpha=.01)),])\n",
    "\n",
    "    evaluateTesting(model, 'pipeline')\n",
    "    print('Finished {}.'.format(block_name))\n",
    "else:\n",
    "    print('Skipped {}.'.format(block_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c5e8b82eac13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mlog_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlog_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_name' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import locale\n",
    "from datetime import datetime as dt\n",
    "\n",
    "#log = False\n",
    "#log_name = ''\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'FR')\n",
    "\n",
    "filename = dt.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if log_name:\n",
    "    filename = filename + '-' + log_name\n",
    "\n",
    "# Function for easily adding Accuracy scores to the new DataFrame row.\n",
    "def AddResultToDF(algorithm, testing_type):\n",
    "    if algorithm in logged_results and testing_type in logged_results[algorithm] and 'accuracy' in logged_results[algorithm][testing_type]:\n",
    "        accuracy = logged_results[algorithm][testing_type]['accuracy']\n",
    "        return locale.format('%.2f', (accuracy * 100.0))\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "if log:\n",
    "    # Add JSON file containing input parameters and results to test-logs folder.\n",
    "    output_json = {}\n",
    "    output_json[\"input_parameters\"] = args\n",
    "    output_json[\"model_test_results\"] = logged_results\n",
    "\n",
    "    with open(\"test-logs/{}.json\".format(filename), \"w\") as outfile:\n",
    "        json.dump(output_json, outfile, indent=2)\n",
    "    \n",
    "    # Add results to generated-test-reports Excel file.\n",
    "    log_df = pd.read_excel(\"test-logs/generated-test-reports.xlsx\")\n",
    "    log_df.loc[len(log_df.index)] = [\n",
    "        filename,\n",
    "        dt.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        AddResultToDF('naive_bayes', 'split'),\n",
    "        AddResultToDF('naive_bayes', '5_fold_cv'),\n",
    "        AddResultToDF('svm', 'split'),\n",
    "        AddResultToDF('svm', '5_fold_cv'),\n",
    "        AddResultToDF('pipeline', 'split'),\n",
    "        AddResultToDF('pipeline', '5_fold_cv'),\n",
    "        ''\n",
    "    ]\n",
    "    log_df.to_excel(\"test-logs/generated-test-reports.xlsx\", index=False, float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
